{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvN0IDpIPQdEk7u0FG3fxh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santapaji/NBC_for_rotten_tomatoes/blob/main/DM_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/rt_reviews.csv\", encoding='iso-8859-1')\n",
        "\n",
        "print(df)\n",
        "# Divide the dataset into train, development, and test sets\n",
        "train_df = df.sample(frac=0.6, random_state=42)\n",
        "dev_df = df.drop(train_df.index).sample(frac=0.5, random_state=42)\n",
        "test_df = df.drop(train_df.index).drop(dev_df.index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYsACiJshOS7",
        "outputId": "8dac20c3-b8f8-4b52-c863-1feea6a26794"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Freshness                                             Review\n",
            "0          fresh   Manakamana doesn't answer any questions, yet ...\n",
            "1          fresh   Wilfully offensive and powered by a chest-thu...\n",
            "2         rotten   It would be difficult to imagine material mor...\n",
            "3         rotten   Despite the gusto its star brings to the role...\n",
            "4         rotten   If there was a good idea at the core of this ...\n",
            "...          ...                                                ...\n",
            "479995    rotten   Zemeckis seems unable to admit that the motio...\n",
            "479996     fresh   Movies like The Kids Are All Right -- beautif...\n",
            "479997    rotten   Film-savvy audiences soon will catch onto Win...\n",
            "479998     fresh                        An odd yet enjoyable film. \n",
            "479999     fresh   No other animation studio, even our beloved P...\n",
            "\n",
            "[480000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Build the vocabulary\n",
        "# Set the threshold for rare words\n",
        "threshold = 10\n",
        "\n",
        "# Build the vocabulary\n",
        "word_counts = Counter()\n",
        "for i, row in train_df.iterrows():\n",
        "    words = row[\"tokens\"]\n",
        "    word_counts.update(words)\n",
        "\n",
        "vocab = [w for w, c in word_counts.items() if c >= threshold]\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n"
      ],
      "metadata": {
        "id": "DGBaqq3Yw08p"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download the required NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Tokenization\n",
        "train_df[\"tokens\"] = train_df[\"Review\"].apply(nltk.word_tokenize)\n",
        "\n",
        "# Stop Word Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "train_df[\"tokens\"] = train_df[\"tokens\"].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "train_df[\"tokens\"] = train_df[\"tokens\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "train_df[\"tokens\"] = train_df[\"tokens\"].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7gB7EC83HrD",
        "outputId": "e939382b-6bb8-4a99-982c-d50267c43e2a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Calculate the prior probabilities\n",
        "num_docs = len(train_df)\n",
        "num_fresh = len(train_df[train_df[\"Freshness\"] == \"fresh\"])\n",
        "num_rotten = len(train_df[train_df[\"Freshness\"] == \"rotten\"])\n",
        "p_fresh = num_fresh / num_docs\n",
        "p_rotten = num_rotten / num_docs\n",
        "\n",
        "# Batch Processing\n",
        "batch_size = 10000\n",
        "word_counts_fresh = np.zeros(len(vocab), dtype=int)\n",
        "word_counts_rotten = np.zeros(len(vocab), dtype=int)\n",
        "for i in range(math.ceil(num_docs/batch_size)):\n",
        "    batch_df = train_df.iloc[i*batch_size:(i+1)*batch_size]\n",
        "    for j, row in batch_df.iterrows():\n",
        "        tokens = row[\"tokens\"]\n",
        "        if row[\"Freshness\"] == \"fresh\":\n",
        "            word_counts_fresh[[word2idx[w] for w in tokens if w in vocab]] += 1\n",
        "        else:\n",
        "            word_counts_rotten[[word2idx[w] for w in tokens if w in vocab]] += 1\n",
        "\n",
        "alpha = 1.0 # smoothing parameter\n",
        "p_word_fresh = (word_counts_fresh + alpha) / (num_fresh + alpha * len(vocab))\n",
        "p_word_rotten = (word_counts_rotten + alpha) / (num_rotten + alpha * len(vocab))\n"
      ],
      "metadata": {
        "id": "cMHamRwvxyqO"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_review(review):\n",
        "    # Split the review into words and convert to lowercase\n",
        "    words = review.lower().split()\n",
        "\n",
        "    # Calculate the log-likelihood of each class\n",
        "    log_p_fresh = np.log(p_fresh) + np.sum(np.log(p_word_fresh[[word2idx[w] for w in words if w in vocab]]))\n",
        "    log_p_rotten = np.log(p_rotten) + np.sum(np.log(p_word_rotten[[word2idx[w] for w in words if w in vocab]]))\n",
        "\n",
        "    # Return the predicted class\n",
        "    if log_p_fresh > log_p_rotten:\n",
        "        return \"fresh\"\n",
        "    else:\n",
        "        return \"rotten\"\n",
        "        \n",
        "# Test the NBC on a sample review\n",
        "review = \"amazing movie totally worth it .\"\n",
        "predicted_class = predict_review(review)\n",
        "print(predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_Xykzf0x6tt",
        "outputId": "7d382aab-58eb-40a2-86b6-bfd001864efa"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fresh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(df):\n",
        "    y_true = df[\"Freshness\"]\n",
        "    y_pred = df[\"Review\"].apply(predict_review)\n",
        "    \n",
        "    tp = ((y_true == \"fresh\") & (y_pred == \"fresh\")).sum()\n",
        "    fp = ((y_true == \"rotten\") & (y_pred == \"fresh\")).sum()\n",
        "    fn = ((y_true == \"fresh\") & (y_pred == \"rotten\")).sum()\n",
        "    tn = ((y_true == \"rotten\") & (y_pred == \"rotten\")).sum()\n",
        "    \n",
        "    accuracy = (tp + tn) / len(df)\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1_score = 2 * precision * recall / (precision + recall)\n",
        "    \n",
        "    return accuracy, precision, recall, f1_score\n",
        "    \n",
        "# Evaluate the NBC on the development set\n",
        "accuracy, precision, recall, f1_score = evaluate(dev_df)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-score: {f1_score:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV7hLxtaBsR2",
        "outputId": "b5f1fcdb-e543-4a5e-d604-7cddffe72265"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.63\n",
            "Precision: 0.60\n",
            "Recall: 0.78\n",
            "F1-score: 0.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_words_fresh = [vocab[i] for i in np.argsort(-p_word_fresh)[:10]]\n",
        "top_words_rotten = [vocab[i] for i in np.argsort(-p_word_rotten)[:10]]\n",
        "print(\"Top 10 words that predict freshness:\", top_words_fresh)\n",
        "print(\"Top 10 words that predict rottenness:\", top_words_rotten)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-Ln6RqEIWar",
        "outputId": "38bbef93-e44f-44f7-e3aa-f6dc8976c379"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 words that predict freshness: ['.', ',', \"'s\", 'film', 'movi', 'one', \"n't\", '...', 'make', 'like']\n",
            "Top 10 words that predict rottenness: ['.', ',', \"'s\", 'film', 'movi', \"n't\", 'like', 'one', '...', \"''\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "alphas = np.logspace(-3, 3, num=7)\n",
        "accuracies = []\n",
        "for alpha in alphas:\n",
        "    # Calculate the conditional probabilities with smoothing\n",
        "    p_word_fresh = (word_counts_fresh + alpha) / (num_fresh + alpha * len(vocab))\n",
        "    p_word_rotten = (word_counts_rotten + alpha) / (num_rotten + alpha * len(vocab))\n",
        "\n",
        "    # Train the NBC with smoothing\n",
        "    def predict_review(review):\n",
        "        words = review.lower().split()\n",
        "        log_p_fresh = np.log(p_fresh) + np.sum(np.log(p_word_fresh[[word2idx[w] for w in words if w in vocab]]))\n",
        "        log_p_rotten = np.log(p_rotten) + np.sum(np.log(p_word_rotten[[word2idx[w] for w in words if w in vocab]]))\n",
        "        if log_p_fresh > log_p_rotten:\n",
        "            return \"fresh\"\n",
        "        else:\n",
        "            return \"rotten\"\n",
        "    accuracy, _, _, _ = evaluate(dev_df)\n",
        "    accuracies.append(accuracy)\n",
        "    \n",
        "plt.semilogx(alphas, accuracies, \"bo-\")\n",
        "plt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "sMCyzxSWIXoG",
        "outputId": "3befef93-0ae6-4dc8-e276-c70f292f6042"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-e850bc2f9cba>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"rotten\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-64fd8ec955a2>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Freshness\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fresh\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fresh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[0;32m-> 4771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m     def _reduce(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-e850bc2f9cba>\u001b[0m in \u001b[0;36mpredict_review\u001b[0;34m(review)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlog_p_fresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_fresh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_word_fresh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mlog_p_rotten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_rotten\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_word_rotten\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog_p_fresh\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlog_p_rotten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-85-e850bc2f9cba>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlog_p_fresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_fresh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_word_fresh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mlog_p_rotten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_rotten\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_word_rotten\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog_p_fresh\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlog_p_rotten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-VtG4qIIp7_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}